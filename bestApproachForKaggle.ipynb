{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install_deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q catboost optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier\n",
    "import optuna\n",
    "\n",
    "DATA_DIR = \"/kaggle/input/playground-series-s6e2\"\n",
    "OUTPUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "CONFIG = {\n",
    "    'batch_size': 1024,\n",
    "    'lr': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'epochs': 50,\n",
    "    'patience': 10,\n",
    "    'device': torch.device(\"cpu\"),\n",
    "    'seed': 42\n",
    "}\n",
    "print('Using device:', CONFIG['device'])\n",
    "\n",
    "FEATURES = {\n",
    "    'continuous': ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression'],\n",
    "    'ordinal': ['Chest pain type', 'EKG results', 'Slope of ST', 'Number of vessels fluro', 'Thallium'],\n",
    "    'binary': ['Sex', 'FBS over 120', 'Exercise angina'],\n",
    "    'target': 'Heart Disease'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicEmbedding(nn.Module):\n",
    "    def __init__(self, frequency_num=16, output_dim=8, sigma=0.1):\n",
    "        super().__init__()\n",
    "        self.k = frequency_num\n",
    "        self.c = nn.Parameter(torch.randn(frequency_num) * sigma)\n",
    "        self.linear = nn.Linear(frequency_num * 2, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        v = 2 * math.pi * self.c * x\n",
    "        out = torch.cat([torch.sin(v), torch.cos(v)], dim=1) \n",
    "        out = self.linear(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class PiecewiseLinearEmbedding(nn.Module):\n",
    "    def __init__(self, bin_edges, output_dim=4):\n",
    "        super().__init__()\n",
    "        self.register_buffer('bin_edges', bin_edges)\n",
    "        num_bins = len(bin_edges) - 1\n",
    "        self.linear = nn.Linear(num_bins, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        edges = self.bin_edges\n",
    "        widths = edges[1:] - edges[:-1]\n",
    "        lower = edges[:-1]\n",
    "        x_expanded = x - lower\n",
    "        encoding = x_expanded / (widths + 1e-6)\n",
    "        encoding = torch.clamp(encoding, 0.0, 1.0)\n",
    "        out = self.linear(encoding)\n",
    "        return out\n",
    "\n",
    "class TabularHeartModel(nn.Module):\n",
    "    def __init__(self, ordinal_edges_dict):\n",
    "        super().__init__()\n",
    "        self.cont_embeddings = nn.ModuleDict()\n",
    "        for feat in FEATURES['continuous']:\n",
    "            self.cont_embeddings[feat] = PeriodicEmbedding(frequency_num=16, output_dim=8, sigma=0.1)\n",
    "            \n",
    "        self.ord_embeddings = nn.ModuleDict()\n",
    "        for feat in FEATURES['ordinal']:\n",
    "            edges = ordinal_edges_dict[feat]\n",
    "            self.ord_embeddings[feat] = PiecewiseLinearEmbedding(bin_edges=edges, output_dim=4)\n",
    "            \n",
    "        input_dim = (len(FEATURES['continuous']) * 8) + \\\n",
    "                    (len(FEATURES['ordinal']) * 4) + \\\n",
    "                    len(FEATURES['binary'])\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x_cont, x_ord, x_bin):\n",
    "        embeddings = []\n",
    "        for i, feat_name in enumerate(FEATURES['continuous']):\n",
    "            val = x_cont[:, i:i+1]\n",
    "            emb = self.cont_embeddings[feat_name](val)\n",
    "            embeddings.append(emb)\n",
    "            \n",
    "        for i, feat_name in enumerate(FEATURES['ordinal']):\n",
    "            val = x_ord[:, i:i+1]\n",
    "            emb = self.ord_embeddings[feat_name](val)\n",
    "            embeddings.append(emb)\n",
    "            \n",
    "        embeddings.append(x_bin)\n",
    "        x = torch.cat(embeddings, dim=1)\n",
    "        return x, self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, df, feature_groups):\n",
    "        self.df = df\n",
    "        self.feats = feature_groups\n",
    "        self.cont_data = df[self.feats['continuous']].values.astype(np.float32)\n",
    "        self.ord_data = df[self.feats['ordinal']].values.astype(np.float32)\n",
    "        self.bin_data = df[self.feats['binary']].values.astype(np.float32)\n",
    "        \n",
    "        if self.feats['target'] in df.columns:\n",
    "            self.labels = df[self.feats['target']].values.astype(np.float32).reshape(-1, 1)\n",
    "        else:\n",
    "            self.labels = np.zeros((len(df), 1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'cont': torch.tensor(self.cont_data[idx]),\n",
    "            'ord': torch.tensor(self.ord_data[idx]),\n",
    "            'bin': torch.tensor(self.bin_data[idx]),\n",
    "            'label': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "def prepare_data():\n",
    "    train_full = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "    test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n",
    "    \n",
    "    train_full['Heart Disease'] = train_full['Heart Disease'].map({'Absence': 0, 'Presence': 1})\n",
    "    \n",
    "    train_df, val_df = train_test_split(train_full, test_size=0.2, random_state=42, stratify=train_full['Heart Disease'])\n",
    "    \n",
    "    ordinal_edges = {}\n",
    "    for col in FEATURES['ordinal']:\n",
    "        edges = np.quantile(train_df[col].dropna(), np.linspace(0, 1, 9))\n",
    "        if len(np.unique(edges)) < len(edges):\n",
    "            edges = np.unique(edges)\n",
    "        ordinal_edges[col] = torch.tensor(edges, dtype=torch.float32)\n",
    "        \n",
    "    return train_df, val_df, test_df, ordinal_edges\n",
    "\n",
    "train_df, val_df, test_df, ordinal_edges = prepare_data()\n",
    "\n",
    "train_dataset = HeartDataset(train_df, FEATURES)\n",
    "val_dataset = HeartDataset(val_df, FEATURES)\n",
    "test_dataset = HeartDataset(test_df, FEATURES)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embed_cache",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(loader, model, device):\n",
    "    model.eval()\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            b_cont = batch['cont'].to(device)\n",
    "            b_ord = batch['ord'].to(device)\n",
    "            b_bin = batch['bin'].to(device)\n",
    "            \n",
    "            features, _ = model(b_cont, b_ord, b_bin)\n",
    "            embeddings_list.append(features.cpu().numpy())\n",
    "            labels_list.append(batch['label'].numpy())\n",
    "            \n",
    "    return np.vstack(embeddings_list), np.vstack(labels_list).ravel()\n",
    "\n",
    "emb_train_path = os.path.join(OUTPUT_DIR, 'X_train_emb.npy')\n",
    "emb_val_path = os.path.join(OUTPUT_DIR, 'X_val_emb.npy')\n",
    "emb_test_path = os.path.join(OUTPUT_DIR, 'X_test_emb.npy')\n",
    "y_train_path = os.path.join(OUTPUT_DIR, 'y_train_emb.npy')\n",
    "y_val_path = os.path.join(OUTPUT_DIR, 'y_val_emb.npy')\n",
    "\n",
    "if os.path.exists(emb_train_path) and os.path.exists(emb_test_path):\n",
    "    print(\"Loading cached embeddings...\")\n",
    "    X_train_emb = np.load(emb_train_path)\n",
    "    X_val_emb = np.load(emb_val_path)\n",
    "    X_test_emb = np.load(emb_test_path)\n",
    "    y_train_emb = np.load(y_train_path)\n",
    "    y_val_emb = np.load(y_val_path)\n",
    "else:\n",
    "    print(\"Training Embeddings...\")\n",
    "    model = TabularHeartModel(ordinal_edges).to(CONFIG['device'])\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    model_save_path = os.path.join(OUTPUT_DIR, 'best_model_cat.pth')\n",
    "\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            b_cont = batch['cont'].to(CONFIG['device'])\n",
    "            b_ord = batch['ord'].to(CONFIG['device'])\n",
    "            b_bin = batch['bin'].to(CONFIG['device'])\n",
    "            labels = batch['label'].to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            _, logits = model(b_cont, b_ord, b_bin)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                b_cont = batch['cont'].to(CONFIG['device'])\n",
    "                b_ord = batch['ord'].to(CONFIG['device'])\n",
    "                b_bin = batch['bin'].to(CONFIG['device'])\n",
    "                labels = batch['label'].to(CONFIG['device'])\n",
    "                _, logits = model(b_cont, b_ord, b_bin)\n",
    "                val_loss += criterion(logits, labels).item()\n",
    "                \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                break\n",
    "                \n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "    \n",
    "    X_train_emb, y_train_emb = extract_embeddings(train_loader, model, CONFIG['device'])\n",
    "    X_val_emb, y_val_emb = extract_embeddings(val_loader, model, CONFIG['device'])\n",
    "    X_test_emb, _ = extract_embeddings(test_loader, model, CONFIG['device'])\n",
    "    \n",
    "    np.save(emb_train_path, X_train_emb)\n",
    "    np.save(emb_val_path, X_val_emb)\n",
    "    np.save(emb_test_path, X_test_emb)\n",
    "    np.save(y_train_path, y_train_emb)\n",
    "    np.save(y_val_path, y_val_emb)\n",
    "    print(\"Embedding Training Complete and Cached.\")\n",
    "\n",
    "X_full = np.concatenate([X_train_emb, X_val_emb], axis=0)\n",
    "y_full = np.concatenate([y_train_emb, y_val_emb], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search_space(trial, stage_params=None):\n",
    "    if stage_params is None: \n",
    "        return {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0),\n",
    "            \"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]),\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", 1e-8, 10.0, log=True),\n",
    "            \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 10.0) if trial.params.get(\"bootstrap_type\") == \"Bayesian\" else None,\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.1, 1.0) if trial.params.get(\"bootstrap_type\") != \"Bayesian\" else None,\n",
    "        }\n",
    "    else:\n",
    "        p = stage_params\n",
    "        bs_type = p[\"bootstrap_type\"]\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", max(1e-4, p[\"learning_rate\"]*0.8), min(0.3, p[\"learning_rate\"]*1.2), log=True),\n",
    "            \"depth\": trial.suggest_int(\"depth\", max(3, p[\"depth\"]-1), min(12, p[\"depth\"]+1)),\n",
    "            \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", max(0.1, p[\"l2_leaf_reg\"]*0.8), min(20, p[\"l2_leaf_reg\"]*1.2)),\n",
    "            \"bootstrap_type\": bs_type,\n",
    "            \"random_strength\": trial.suggest_float(\"random_strength\", max(1e-9, p[\"random_strength\"]*0.8), min(10.0, p[\"random_strength\"]*1.2), log=True),\n",
    "        }\n",
    "        if bs_type == \"Bayesian\":\n",
    "            params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", max(0.0, p.get(\"bagging_temperature\", 1)*0.8), min(10.0, p.get(\"bagging_temperature\", 1)*1.2))\n",
    "        else:\n",
    "            params[\"subsample\"] = trial.suggest_float(\"subsample\", max(0.1, p.get(\"subsample\", 0.8)*0.9), min(1.0, p.get(\"subsample\", 0.8)*1.1))\n",
    "        return params\n",
    "\n",
    "def objective(trial, stage_params=None):\n",
    "    params = get_search_space(trial, stage_params)\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "    \n",
    "    params.update({\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": 50,\n",
    "        \"verbose\": False,\n",
    "        \"allow_writing_files\": False,\n",
    "        \"random_seed\": 42,\n",
    "        \"thread_count\": 2,\n",
    "        \"task_type\": \"CPU\",\n",
    "        \"iterations\": 500\n",
    "    })\n",
    "    \n",
    "    fold_aucs = []\n",
    "    skf_optuna = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold_i, (train_idx, val_idx) in enumerate(skf_optuna.split(X_full, y_full)):\n",
    "        X_tr, y_tr = X_full[train_idx], y_full[train_idx]\n",
    "        X_va, y_va = X_full[val_idx], y_full[val_idx]\n",
    "        \n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n",
    "        \n",
    "        if fold_i == 0:\n",
    "            evals = model.get_evals_result()[\"validation\"][\"AUC\"]\n",
    "            for step, auc in enumerate(evals):\n",
    "                trial.report(auc, step)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        best_auc = model.get_best_score()['validation']['AUC']\n",
    "        fold_aucs.append(best_auc)\n",
    "        \n",
    "    return np.mean(fold_aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_stage1",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_stage1 = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(\n",
    "        multivariate=True, \n",
    "        n_startup_trials=50, \n",
    "        gamma=lambda n: int(0.25 * n)\n",
    "    ),\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=20, \n",
    "        n_warmup_steps=30\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Starting Stage 1\")\n",
    "study_stage1.optimize(lambda t: objective(t, stage_params=None), n_trials=100)\n",
    "print(f\"Stage 1 Best AUC: {study_stage1.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optuna_stage2",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_stage2 = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(\n",
    "        multivariate=True, \n",
    "        n_startup_trials=20, \n",
    "        gamma=lambda n: int(0.08 * n)\n",
    "    ),\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=10, \n",
    "        n_warmup_steps=20\n",
    "    )\n",
    ")\n",
    "\n",
    "study_stage2.add_trials(study_stage1.trials)\n",
    "\n",
    "print(\"Starting Stage 2\")\n",
    "study_stage2.optimize(lambda t: objective(t, stage_params=study_stage1.best_params), n_trials=80)\n",
    "print(f\"Stage 2 Best AUC: {study_stage2.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_params = study_stage2.best_params.copy()\n",
    "\n",
    "final_params.update({\n",
    "    \"iterations\": 8000, \n",
    "    \"verbose\": 1000, \n",
    "    \"eval_metric\": \"AUC\",\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 200, \n",
    "    \"allow_writing_files\": False,\n",
    "    \"thread_count\": 2,\n",
    "    \"task_type\": \"CPU\"\n",
    "})\n",
    "\n",
    "seeds = [42, 2024, 999]\n",
    "test_preds_ensemble = np.zeros(len(test_df)) \n",
    "oof_preds = np.zeros(len(X_full))\n",
    "oof_targets = y_full\n",
    "\n",
    "for seed in seeds:\n",
    "    final_params[\"random_seed\"] = seed\n",
    "    print(f\"--> Processing Seed: {seed}\")\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    \n",
    "    seed_oof = np.zeros(len(X_full))\n",
    "    seed_test_preds = np.zeros(len(test_df))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_full, y_full)):\n",
    "        X_tr, y_tr = X_full[train_idx], y_full[train_idx]\n",
    "        X_va, y_va = X_full[val_idx], y_full[val_idx]\n",
    "        \n",
    "        clf = CatBoostClassifier(**final_params)\n",
    "        clf.fit(X_tr, y_tr, eval_set=(X_va, y_va), use_best_model=True)\n",
    "        \n",
    "        val_p = clf.predict_proba(X_va)[:, 1]\n",
    "        seed_oof[val_idx] = val_p\n",
    "        \n",
    "        test_p = clf.predict_proba(X_test_emb)[:, 1]\n",
    "        seed_test_preds += (test_p / 5)\n",
    "\n",
    "    oof_preds += (seed_oof / len(seeds))\n",
    "    test_preds_ensemble += (seed_test_preds / len(seeds))\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'Heart Disease': test_preds_ensemble\n",
    "})\n",
    "submission.to_csv(os.path.join(OUTPUT_DIR, 'submission.csv'), index=False)\n",
    "print(\"Ensemble Submission Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plotting",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "sns.histplot(x=oof_preds, hue=oof_targets, bins=50, kde=True, palette={0: \"blue\", 1: \"red\"}, alpha=0.5)\n",
    "\n",
    "plt.title(f\"Predicted Probabilities vs Actual Labels (OOF AUC: {roc_auc_score(oof_targets, oof_preds):.5f})\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend(title=\"Actual Target\", labels=[\"Presence (1)\", \"Absence (0)\"])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}